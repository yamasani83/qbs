{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People & Places\n",
    "## Named Entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will learn how to extract named entities (names of people, places, groups, institutions, etc.) from text files and then analyze the results.\n",
    "\n",
    "For example, one of our goals is to create a map of all countries mentioned in the State of the Union corpus. \n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">What steps do you anticipate we will have to do in order to successfully accomplish this project? List the steps in the markdown below:</p></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setup\n",
    "\n",
    "1. Install **spaCy**\n",
    "\n",
    "From command line / terminal - see https://spacy.io/usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In Python, import the necessary packages for today's lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "import pandas as pd\n",
    "from spacy.lang.en.examples import sentences\n",
    "from spacy import displacy   #for visualizing word types and relationships"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Import the necessary language models. For documentation on **spaCy**'s four available language models, see: https://spacy.io/models/en. Note, the accuracy scores for the small, medium and large models are all roughly similar, thus it makes sense to use the small model. The TRF model does score a little better, but given its size, we will stick with the small model.\n",
    "\n",
    "For more on spaCy's other models see: https://spacy.io/models.Note: many of these models are trained on 21st-century online new media and websites like Wikipedia. Thus, the accuracy of its NLP methods will decline as the texts on which you apply them differ from the texts the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Apply NER to sample sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's start by experimenting with some preloaded sample sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">5. Before we use spaCy's Named Entity Recognizer, can you identify all examples of the three principle types of named entities (person names, place names, and group / organization names) found in the above sentences? Type your answer below.</p></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Before we get to NER, let's identify [**parts of speech**](https://spacy.io/usage/linguistic-features#pos-tagging) (often abbreviated as \"POS\") and **dependencies** (i.e. who/what is doing what action to whom (or what thing)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes the text & then classifies each token in a variety of categories (pos, ner, dependencies, etc.)\n",
    "doc = nlp(sentences[0]) \n",
    "print(doc.text)  #prints the whole sentence\n",
    "for token in doc:\n",
    "    #for each token, prints the token, its POS tag, and dependency tag\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We can also identify:\n",
    "+ stop words (words that don't reveal much about content)\n",
    "+ the lemmatized version of words\n",
    "+ type of token (*alpha*: is it all letters, or does it include punctuation, numbers, and special symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Below, we extract named entities from this same sentence. [Click here for more on spaCy's NER functions](https://spacy.io/usage/linguistic-features#named-entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. As you can see, spaCy's NER identifies more than the basic people, places and organizations. To identify what these labels mean, you can:\n",
    "+ Look up the \"Label Scheme\" documentation on the [model page](https://spacy.io/models/en).\n",
    "+ view the [full glossary](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py) of terms\n",
    "+ get info for one term using spacy.explain(), *see below*:\n",
    "\n",
    "*I also provide additional information in the next markdown cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic named entity recognizers commonly identify the following types of entities:\n",
    "\n",
    "```\n",
    "place names\n",
    "person names\n",
    "group names\n",
    "miscellaneous / other entities\n",
    "```\n",
    "\n",
    "**spaCy**'s NER identifies a wider-range of entities.\n",
    "\n",
    "Examine the list of entity types identified by spaCy below (from the [spaCY glossary](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)):\n",
    "\n",
    "```\n",
    "PERSON:      People, including fictional.\n",
    "NORP:        Nationalities or religious or political groups.\n",
    "FAC:         Buildings, airports, highways, bridges, etc.\n",
    "ORG:         Companies, agencies, institutions, etc.\n",
    "GPE:         Countries, cities, states.\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART: Titles of books, songs, etc.\n",
    "LAW:         Named documents made into laws.\n",
    "LANGUAGE:    Any named language.\n",
    "DATE:        Absolute or relative dates or periods.\n",
    "TIME:        Times smaller than a day.\n",
    "PERCENT:     Percentage, including ”%“.\n",
    "MONEY:       Monetary values, including unit.\n",
    "QUANTITY:    Measurements, as of weight or distance.\n",
    "ORDINAL:     “first”, “second”, etc.\n",
    "CARDINAL:    Numerals that do not fall under another type.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now, let's apply spaCy's NER to the full list of sample sentences. Do you see any errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('. '.join(sentences)) \n",
    "print(doc.text)  \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Visualize spaCy tagging using displacy\n",
    "\n",
    "11. We can use spaCy's **displacy** module to visualize some of these linguistic labels. For more, see: https://spacy.io/usage/visualizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = \"dep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">12. In the cells below, write your own series of sentences (maybe 4-6), read them into spacy (using the **nlp** function), extract named entities from these sentences, and then display the results using **displacy**. (If time allows, you can also examine POS and other tags as well.) Try to make it challenging for spaCy. What does it do well? Where does it fail?</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your own NER model\n",
    "\n",
    "As with all examples of machine learning, these models work better when trained on the same type of data they will be tested on. So, if you are analyzing medical records, legal trial transcripts, or historical documents, your model will not do well if it was trained on Wikipedia or news articles.\n",
    "\n",
    "**spaCy** allows you to train your own model or modify existing models. This is beyond the scope of this lesson, but to learn more see their [Training Pipelines & Models](https://spacy.io/usage/training) documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Apply NER to a full text\n",
    "\n",
    "### Extracting entities from Biden's 2023 State of the Union address\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">13. Given what we've learned so far about NER, in the markdown cell below, brainstorm some different research questions you could use spaCy's NER to help you answer (given the list of entities it could help you analyze) about the State of the Union addresses that we have been studying.</p></div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">13b. Next, brainstorm and discuss the following:</p>\n",
    "<ul>\n",
    "    <li style=\"color:green\">What texts or types of texts are you interested in analyzing?</li>\n",
    "    <li style=\"color:green\">How could you apply NER to these texts? What questions could you answer?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IVa. Reading in SOTU addresses as a dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. In the last lesson, we learned about some pre-processing tasks that are essential to many types of more advanced NLP methods and to answer many common text analysis questions. These tasks include:\n",
    "+ counting words in each text\n",
    "+ tokenizing texts\n",
    "+ lower-casing tokens\n",
    "+ removing stopwords \n",
    "\n",
    "In this case, we have a dataset of State of the Union (SOTU) addresses already pre-processed for us. In this case, we saved the dataset as a \".tsv\" to indicate it is a tab-separated-values file rather than a \"csv\" or comma-separated-values file. Thus, on import, we need to indicate the separator (aka. \"delimiter\") is a tab (\"\\t\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the .tsv as a dataframe\n",
    "sotudf = pd.read_csv(\"sotudf.tsv\", encoding=\"utf-8\", sep=\"\\t\", index_col=0)\n",
    "sotudf = sotudf.sort_values(by = ['year']) #you can probably guess what this does\n",
    "sotudf.tail()  #outputs last 5 rows in dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">15. Can you identify what is contained in each column or field in this dataset?</p>\n",
    "\n",
    "16. Next, we want to look at the full text of the most recent SOTU address. We can do that by calling the row which contains the address (Biden 2023) and the \"fulltext\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biden23text = sotudf.iloc[-1, 4]\n",
    "biden23text = sotudf[sotudf['year'] == 2023]['fulltext'].item()\n",
    "biden23text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(biden23text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. We can store entity information in a list (denoted by \"[]\") of tuples (denoted by \"()\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\n",
    "print(ents[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. We can iterate through this list of entities from our SOTU address and then save select types of named entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_names = []\n",
    "for ent in ents:\n",
    "    if ent[1] == \"PERSON\":\n",
    "        person_names.append(ent[0])\n",
    "person_names[:10]\n",
    "\n",
    "#list comprehension to produce the same results in one line of code:\n",
    "#person_names = [ent[0] for ent in ents if ent[1] == \"PERSON\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><p style=\"color:green\">19. Re-run the code above, but this time choosing another entity type to extract.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Note, to get place names, you need to request more than one entity type. Try running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_names = [(ent[0], ent[1]) for ent in ents if ent[1] in ['GPE', 'LOC']]\n",
    "place_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice any mislabeled place names above? If so, they may be **false positives** (not named entities at all) or misidentified NEs (i.e. labeling a place name as a person). If you want to assess the accuracy of NERs, however, you should also examine the number of **false negatives** (named entities not recognized by the model).\n",
    "\n",
    "21. To analyze the results, it is often helpful to create a frequency list. Python's **collections** library makes that fairly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnfreqs = collections.Counter(place_names)\n",
    "pnfreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnfreqs.most_common(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. A common issue with NER is that many named entities are referred to by multiple names (\"America\", \"The United States\", \"The United States of America\", and \"USA\"/\"U.S.A.\" or \"Samuel L. Jackson\", \"Samuel Jackson\", \"Sam L. Jackson\", \"Sam Jackson\", etc.), have different names in different languages (\"London\"/ \"Londres, \"Roma\" / \"Rome\"), or can sometimes be referred to by their title instead of their name (\"The President\", \"The Captain\"). \n",
    "\n",
    "There is no easy solution to this. Usually the preferred way to deal with multiple aliases is to create a separate dictionary with a unique identifier (i.e. \"S66M23001A\"), the standardized form of the name, and a list of potential aliases. Fortunately, there are some existing Python packages that help with well-known entities like countries. \n",
    "\n",
    "In the following notebook \"NLP2_NER2_MappingCountries.ipynb\" we will get some practice doing just that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
