{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis in Python 1: Working with Strings & Files\n",
    "\n",
    "<h1 style=\"text-align:center;font-size:300%;\">The State of the Union is ... long?</h1> \n",
    "  <!--<img src=\"https://miro.medium.com/max/720/1*pp7HX01jBv2wbVRW9Ml_mA.png\" style=\"width:%80;\">-->\n",
    "  <img src = \"https://cdn.theatlantic.com/thumbor/7G7_MhUOYg6M8JGSmbQCVoaJ-kY=/126x0:1794x938/1536x864/media/img/2015/01/16/opener_words/original.jpg\">\n",
    "\n",
    "## INTRODUCTION TO TEXT ANALYSIS IN PYTHON\n",
    "**How can we use computational techniques to analyze texts and then visualize patterns buried within them?** \n",
    "\n",
    "**What can we learn about texts by applying text analysis in Python? How do we get started?**\n",
    "\n",
    "<p>In this session, participants will:</p>\n",
    "\n",
    "+ Learn how to write basic scripts in Python using Jupyter Notebooks\n",
    "+ import and pre-process documents\n",
    "+ analyze each document using word frequencies, collocations, ngram frequencies, etc.\n",
    "\n",
    "**In particular, our goal is to re-create the word frequency graph (see below) found in [Schmidt & Fraas, “The Language of the State of the Union” (The Atlantic, Jan. 18, 2015)](https://www.theatlantic.com/politics/archive/2015/01/the-language-of-the-state-of-the-union/384575/).**\n",
    "\n",
    "![](SOTU_TheAtlantic_barplot.png)\n",
    "\n",
    "  **Before we begin, jot down below (or discuss with your classmates) your answer to the following question: what steps would you need to follow to convert a collection of State of the Union (SOTU) addresses into a bar plot/chart/graph like the one found in the article above?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Tutorial\n",
    "\n",
    "In this tutorial and notebook, you will practice working with a dataset or corpus of a well-known series of texts: the yearly State of the Union addresses given by Presidents of the United States since 1790.\n",
    "\n",
    "For a great resource to learn and gain more practice doing text analysis with Python see the online book: Melanie Walsh, [*Introduction to Cultural Analytics and Python*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html), Version 1 (2021), https://doi.org/10.5281/zenodo.4411250. \n",
    "\n",
    "<img src=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/_static/favicon.ico\" style=\"width:30%\">\n",
    "\n",
    "<!--All sections below labeled with a **MW** comes from this book. Please consider supporting that project if you find it useful.-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part I. Getting Started\n",
    "\n",
    "### Downloading and Saving Dataset(s)\n",
    "\n",
    "We will be working with a corpus of all presidential State of the Union addresses given by U.S. Presidents (1st: 1791, most recent: 2023).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Tutorial\n",
    "\n",
    "In this tutorial and notebook, you will practice working with a dataset or corpus of a well-known series of texts: the yearly State of the Union addresses given by Presidents of the United States since 1790."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Importing Python Packages or Libraries\n",
    "\n",
    "1. Before beginning, we need to import some packages. Often, we need to install and import customized Python packages (sometimes called \"modules\") in addition to the core functions (like **print()**, **len()**, **sum()**, and others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, glob #these packages allow us to navigate through the files on our own computers\n",
    "from pathlib import Path #the pathlib package helps us work with file paths\n",
    "#for more on using pathlib see: https://builtin.com/software-engineering-perspectives/python-pathlib\n",
    "import collections\n",
    "\n",
    "import pandas as pd #for creating and working with dataframes\n",
    "import nltk, re #we can import multiple packages on one line using commas to separate new package names\n",
    "from nltk import RegexpTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, pos_tag, word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#matplotlib and seaborn are used here to create graphs, charts, and other visualizations\n",
    "import matplotlib.pyplot as plt #needed for xticks\n",
    "import seaborn as sns\n",
    "\n",
    "#code below modifies how plots will be shown in this notebook\n",
    "plt.rcParams['figure.figsize'] = [12, 8]  #changes default figure size to make larger plots\n",
    "%config InteractiveShellApp.matplotlib = 'inline'\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "#Press CTRL+Enter to run this codeblock! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b. If the above prints out the error message: \"ModuleNotFoundError: No module named 'nltk'\". Uncomment out the code below (by removing the #) and run it. Then run the above cell of code (with all the import commands) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Navigating through the files in a directory (whether on your computer or on a remote server)\n",
    "\n",
    "<!--1. To work with the State of the Union addresses you downloaded (hereafter: SOTU), we will need to navigate to the folder you placed them in. First, check the \"current working directory\" that Python is working with:-->\n",
    "3. To work with our State of the Union address text corpus, we will need to navigate to the folder we placed them in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Path.cwd())  #prints out the current working directory (where Python will look for files unless we specify otherwise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Your current working directory, printed out in the previous step, is the location where this notebook is saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path.iterdir(Path.cwd()))\n",
    "\n",
    "#Do you see the \"state of the union dataset\" folder in the list below?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, we will look inside the \"state-of-the-union-dataset\" folder containing our corpus of State of the Union speeches (henceforth: SOTU). We can learn something about this dataset simply by examining the titles of the individual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotudir = Path(\"state-of-the-union-dataset\",\"txt\")    \n",
    "#sotudir = Path(Path.cwd().parent.parent, \"state-of-the-union-dataset\",\"txt\")   #REPLACE WITH PROPER PATH TO WHERE YOU SAVE THE SOTU TEXT FILES!!!\n",
    "print(set([item.suffix for item in list(Path.iterdir(sotudir))]))  #get unique suffixes or file extensions in sotudir \n",
    "[item.name for item in list(Path.iterdir(sotudir))] #to get filename only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Reading Files and Examining Their Contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Open one SOTU text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(sotudir,\"2002_Bush.txt\"), encoding='utf-8') as f:\n",
    "    sotu1 = f.read()\n",
    "\n",
    "## ** calling utf-8 encoding may not be necessary\n",
    "## but is good practice if you ever work with foreign languages (besides special characters can appear in English too, as in \n",
    "## loan words like naïve and résumé )\n",
    "\n",
    "\n",
    "##[DISCUSS WHY IT IS GOOD PRACTICE TO CLOSE FILES IMMEDIATELY AFTER YOU ARE DONE WITH THEM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can view the whole text simply by typing the file name\n",
    "sotu1  #Jupyter, however, requires the print() command to print out any information not found in the last line of code in a codeblock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p style=\"color:green\"><b>6b. Code Together</b>: Open a second SOTU address (of your own choosing) and save it into the variable \"sotu2\".</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What do the following blocks of code do? Run them and then share your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sotu1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu1[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu1[:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu1[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu1[-60:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">Exercises for Part III</h3>\n",
    "    \n",
    "<p style=\"color:blue;\">8. Add a coding cell below and print out the first and last 200 characters in your selected <b>sotu2</b> address. Can you identify any major themes from the opening and closing words of this address? If not, expand the number of characters you are examining.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Divide a text into tokens\n",
    "\n",
    "There are different ways to tokenize a text. Two examples from the **Natural Language Toolkit (nltk)** package are provided below:\n",
    "\n",
    "\"Tokens\" can be words, but also punctuation, sets of numbers, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **First**, let's try **nltk**'s **word_tokenize()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sotu1)\n",
    "print(tokens[0:30]) #notice the difference between these tokens and \"rawtokens\" above\n",
    "print(len(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Second**, NLTK offers another way to tokenize, but this time removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to tokenize\n",
    "from nltk import RegexpTokenizer  \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens=tokenizer.tokenize(sotu1)\n",
    "print(tokens[:30])\n",
    "print(len(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">Exercises for Part IV</h3>\n",
    "    \n",
    "<p style=\"color:blue;\">10b. Tokenize your chosen sotu2 text using one of the above two methods.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Words Count / Word Counts\n",
    "\n",
    "11. We can count the length of a text using the **len()** function. Run the following two lines of code below. What is the difference between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sotu1))\n",
    "print(len(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. We can create a frequency list of words in this State of the Union Address using the **Counter()** function from the Python **collections** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokfreqs = collections.Counter(tokens)\n",
    "tokfreqs.most_common(60)         #to view the 60 most common items in tokfreqs\n",
    "#tokfreqs.most_common()[-100:]   #to view the 60 least common items in tokfreqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13, What do you notice? How can we make this frequency list more revealing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. We can convert all the tokens to lower-case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokens = [tok.lower() for tok in tokens]\n",
    "print(\"our tokens2 list converted to lower-case (saved as ltokens2) - contains\",len(ltokens),\"tokens.\")\n",
    "print(ltokens[:40])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. We often want to remove stopwords**. **Stop words** are common words that reveal little about the meaning of a text (such as articles like \"the\", conjunctions like \"and\", prepositions like \"on\", pronouns like \"our\", and helper verbs like \"can\"). Fortunately, NLTK provides a list of stop words in English (and other languages as well) that we can use to eliminate all such words from our texts.\n",
    "\n",
    "Let's examine stopwords in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p style=\"color:green\"><b>Code Together</b>: 16. What if you work with another language? Let's print out the language options for NLTK's stopwords:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages in nltk\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p style=\"color:green\">17. Now try to print out stopwords from a language of your choice (using the same code we used above to print out English stopwords):</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Next, with our English stopwords list, we can further modify our ltokens list by removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop = sorted(stop)\n",
    "ltokens_ns = [tok for tok in ltokens if tok not in stop]        #list comprehension removes all stopwords from ltokens2\n",
    "\n",
    "print(\"We had\",len(ltokens),\"tokens in our ltoken2 list.\")\n",
    "print(\"beginning with:\",ltokens[:30],\" \\n\")\n",
    "print(\"After removing stop words, we now have\",len(ltokens_ns),\"tokens in our list.\")\n",
    "print(\"beginning with:\",ltokens_ns[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. We can then count the frequencies of words in this token list using the **Counter** function from the **collections** library we imported at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokfreqs = collections.Counter(ltokens_ns)\n",
    "tokfreqs.most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p style=\"color:green\"><b>Code Together</b>:</p> \n",
    "    <p style=\"color:green\">20. Using your chosen <b>sotu2</b> address:</p>\n",
    "    <ol style=\"color:green\"> \n",
    "        <li>tokenize it</li> \n",
    "        <li>lower-case it</li> \n",
    "        <li>remove all stopwords</li>\n",
    "        <li>create a frequency count of the words in this address</li> \n",
    "        <li>identify the 30 most frequent tokens (with stopwords removed).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to examine the frequency of multiple-word terms and phrases? \n",
    "\n",
    "Instead of splitting texts into words, we can split them into bigrams (two-word combinations), trigrams (three-word combinations), and other *n*grams (terms with *n* number of words).\n",
    "\n",
    "A great tool for examining the frequency of ngrams over time is [Google's Ngram Viewer.](https://books.google.com/ngrams/) Click on this link and try some different combinations.\n",
    "\n",
    "<!--<h1>\n",
    "    <img src = \"C:\\Users\\F0040RP\\Documents\\Website\\images\\textAnalysis\\google_ngrams_history_subfields.png\" style = \"width: 80%\">\n",
    "    <img src = \"https://books.google.com/ngrams/graph?content=cultural+history%2Cpolitical+history%2Cintellectual+history%2Cmicrohistory%2C+women%27s+history&year_start=1800&year_end=2019&corpus=en-2019&smoothing=3\" style=\"width:80%;\">\n",
    "</h1>-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. To create, count, and analyze ngrams from our own texts and corpora, we can use **NLTK**'s **ngrams** function, which reads in a list of tokens and the number of words we want per combination. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams = list(nltk.ngrams([\"to\", \"be\", \",\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\", \"is\", \"the\", \"question\", \".\"], n))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "n_grams=list(nltk.ngrams(ltokens_ns,n))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(n_grams).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">Exercises for Part VI</h3>\n",
    "    \n",
    "<p style=\"color:blue;\">21b. Create ngrams from sotu2. Try bigrams, trigrams, and even ngrams of length 4. Then identify the most common of these ngrams. Are these results useful to understanding this particular text?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Other NLTK functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = nltk.Text(tokens) #to use many of nltk's functions we need to convert our tokens list into a nltk.Text object\n",
    "    #remember, the tokens variable stores our first set of tokens derived from Washington's 1794 address\n",
    "    ## for concordances, it makes sense to work with unmodified tokens (in original case with all words and punctuation\n",
    "    ## still included)\n",
    "text1.concordance(\"government\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count(\"terror\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.dispersion_plot([\"freedom\", \"God\", \"liberty\", \"war\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"freedom\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">Exercises for Part VII</h3>\n",
    "    \n",
    "<p style=\"color:blue;\">Read the tokens from sotu2 into nltk.Text. Then try applying some of the nltk functions introduced above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
